{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook produces the clustering dataset from the original MIMIC-IV files and some derived MIMIC-IV files (see the README). If these files are in the correct positions then all that is required is running this code. It will output a cleaned csv file in the data directory. The whole notebook takes 5-10 minutes to run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "import os\n",
    "import time\n",
    "import gzip\n",
    "import dask.dataframe as dd\n",
    "import statsmodels.formula.api as smf\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read the data in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** The datasets are large. In this code they are read straight into memory, however, alternate code may be necessary for machines with lower RAM. If it is not possible to run this code directly, we recommend using GoogleBigQuery to produce smaller versions of the chartevents and labevents files. For example, you can query it to only return the instances with the relevant itemids (see how it is read in in this code for more details).\n",
    "\n",
    "First, we expand all of the .gz files. Then, we read in the icu files and the second the hosp files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to expand .gz files\n",
    "def decompress_gz_files(directory, chunk_size=1024*1024):\n",
    "    gz_files = [f for f in os.listdir(directory) if f.endswith('.gz')]\n",
    "    print(f'Found {len(gz_files)} .gz files in the directory.')\n",
    "    if len(gz_files)>0:\n",
    "        print(\"Starting to decompress .gz files...\\n\")\n",
    "\n",
    "    # Loop over all .gz files\n",
    "    for i, filename in enumerate(gz_files, start=1):\n",
    "        original_filepath = os.path.join(directory, filename) # Create a path to the .gz file\n",
    "        new_filepath = os.path.splitext(original_filepath)[0]  # Create a path for the decompressed file (removes .gz from filename)\n",
    "\n",
    "        with gzip.open(original_filepath, 'rb') as f_in:\n",
    "            with open(new_filepath, 'wb') as f_out:\n",
    "                # Read and write the first chunk and measure the time taken\n",
    "                start_time = time.time()\n",
    "                chunk = f_in.read(chunk_size)\n",
    "                f_out.write(chunk)\n",
    "                elapsed_time = time.time() - start_time\n",
    "\n",
    "                # Estimate total time for this file\n",
    "                total_size = os.path.getsize(original_filepath)\n",
    "                estimated_time = total_size / len(chunk) * elapsed_time\n",
    "\n",
    "                print(f'Decompressing {filename} ({i} out of {len(gz_files)}), '\n",
    "                      f'estimated time: {estimated_time:.2f} seconds')\n",
    "\n",
    "                # Read and write the rest of the file in chunks\n",
    "                while chunk := f_in.read(chunk_size):\n",
    "                    f_out.write(chunk)\n",
    "\n",
    "        # Remove the .gz file\n",
    "        os.remove(original_filepath)\n",
    "\n",
    "\n",
    "# Decompress the .gz files\n",
    "decompress_gz_files('data/icu')\n",
    "decompress_gz_files('data/hosp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty dictionary to store icu data\n",
    "icu = {}\n",
    "\n",
    "# Get a list of all CSV files in the icu folder and just the selected ones to import\n",
    "icu_csv_files = [file for file in os.listdir(\"data/icu\") if file.endswith(\".csv\")]\n",
    "icu_files_to_import = ['d_items.csv', 'procedureevents.csv', 'inputevents.csv', 'ingredientevents.csv', 'icustays.csv']\n",
    "\n",
    "# Create dictionary of which columns to read in. Adjust columns for certain large files\n",
    "d_usecols = {file:None for file in icu_files_to_import}\n",
    "d_usecols['ingredientevents.csv'] = ['hadm_id', 'stay_id', 'starttime', 'endtime', 'itemid']\n",
    "d_usecols['inputevents.csv'] = ['hadm_id', 'stay_id', 'starttime', 'endtime', 'itemid', 'ordercategorydescription', 'originalrate']\n",
    "\n",
    "# Create dictionary to parse dates \n",
    "d_parse_dates = {file:None for file in icu_files_to_import}\n",
    "d_parse_dates['ingredientevents.csv'] = ['starttime', 'endtime']\n",
    "d_parse_dates['inputevents.csv'] = ['starttime', 'endtime']\n",
    "\n",
    "# Read each CSV file and store it in a data frame. Note that this ignore chartevents\n",
    "for file_name in icu_files_to_import:\n",
    "\n",
    "    # Identify the path to the file\n",
    "    file_path = os.path.join(\"data/icu\", file_name)\n",
    "\n",
    "    # Remove the \".csv\" extension from the file name\n",
    "    df_name = file_name[:-4]\n",
    "\n",
    "    # Read the CSV file into a data frame\n",
    "    icu[df_name] = pd.read_csv(file_path, usecols = d_usecols[file_name], parse_dates=d_parse_dates[file_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading chartevents to memory. Note that chartevents is a large file so we use dask to read it in chunks and filter it. This might take a while to run depending on your machine.\n",
    "dask_dataframe = dd.read_csv(\"data/icu/chartevents.csv\", dtype={'caregiver_id': 'float64', 'value': 'object', 'valuenum': 'float64', 'warning': 'float64'}, parse_dates=['charttime'])\n",
    "\n",
    "# Define the list of itemids that we need for later. If using GoogleBigQuery to create a smaller dataset, use the following itemids.\n",
    "LAPS_Vars = [\n",
    "    225624, 225624, 227000, 227001, 228152, 229669, 220227, 226767, 226860, 226861, 226862, 226863, 226865, 227035, 228232, 220179,\n",
    "    225309, 226850, 223761, 223762, 220045, 220210, 224688, 224690, 224745, 225475, 227032, 227047, 229425, 229563, 220050, 220052,\n",
    "    220277]\n",
    "\n",
    "# Filter to the itemids in the list and compute\n",
    "icu['chartevents'] = dask_dataframe[dask_dataframe['itemid'].isin(LAPS_Vars)].compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty dictionary to store hosp frames\n",
    "hosp = {}\n",
    "\n",
    "# Get a list of all CSV files in the hosp folder and just the selected ones to import\n",
    "hosp_csv_files = [file for file in os.listdir(\"data/hosp\") if file.endswith(\".csv\")]\n",
    "hosp_files_to_import = ['admissions.csv', 'd_labitems.csv', 'omr.csv', 'diagnoses_icd.csv', 'services.csv', 'patients.csv']\n",
    "\n",
    "# Read each CSV file and store it in a data frame\n",
    "for file_name in hosp_files_to_import:\n",
    "\n",
    "# Identify the path to the file \n",
    "    file_path = os.path.join(\"data/hosp\", file_name)\n",
    "\n",
    "    # Remove the \".csv\" extension from the file name\n",
    "    df_name = file_name[:-4]\n",
    "    \n",
    "    # Read the CSV file into a data frame\n",
    "    hosp[df_name] = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading labevents to memory. Note that labevents is a massive file so we use dask to read it in chunks and filter it. This takes a while to run.\n",
    "dask_dataframe = dd.read_csv(\"data/hosp/labevents.csv\", parse_dates=['charttime'])\n",
    "\n",
    "# Define the list of itemids that we need for later\n",
    "itemid_list = [\n",
    "    50803, 50882, 52039, 50868, 52500, 50824, 50983, 52455, 52623, 50912, 51081, 51977, 52024, 52546,\n",
    "    50820, 52041, 50831, 51094, 51491, 52730, 50813, 52442, 53154, 50824, 50983, 52455, 52623, 50885,\n",
    "    50912, 51081, 51977, 52024, 52546, 50862, 51775, 51776, 51807, 51836, 51910, 51926, 52022, 53085,\n",
    "    53116, 53138, 50809, 50931, 51478, 51981, 52027, 52569, 51221, 51480, 51638, 51639, 52028, 51300,\n",
    "    51301, 51755, 51756, 53134, 50818, 52040, 50821, 52042, 51003, 50825]\n",
    "\n",
    "# Filter to the itemids in the list and compute\n",
    "hosp['labevents'] = dask_dataframe[dask_dataframe['itemid'].isin(itemid_list)].compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature recreation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section contains the code to recreate each feature in Vranas et al. 2017."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Age\n",
    "This code produces the same output as the SQL implementation [here](https://github.com/MIT-LCP/mimic-iv/blob/master/concepts/demographics/age.sql)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'admittime' in admissions dataframe to datetime\n",
    "hosp['admissions']['admittime'] = pd.to_datetime(hosp['admissions']['admittime'])\n",
    "\n",
    "# Merge the dataframes on 'subject_id'\n",
    "merged_df = pd.merge(hosp['admissions'], hosp['patients'], on='subject_id')\n",
    "\n",
    "# Calculate the age at the time of admission\n",
    "merged_df['age'] = merged_df.apply(\n",
    "    lambda row: (row['admittime'].year - row['anchor_year']) + row['anchor_age'], \n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Select the required columns\n",
    "derived_age = merged_df[['hadm_id', 'age']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Comorbidity\n",
    "Using the CCI instead of COPS II (used in Vranas et al. 2017). We use the CCI implementation from the MIMIC-IV Code Repository: [here](https://github.com/MIT-LCP/mimic-code/blob/main/mimic-iv/concepts/comorbidity/charlson.sql). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the CCI data\n",
    "CCI = pd.read_csv(\"data/queried_data/CCI.csv\")[['hadm_id', 'charlson_comorbidity_index']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Emergency department admission\n",
    "This is one of the more complex features in this work. See the Appendix in the paper for details about why we chose this measure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert ED admission and outtime to datetime. Make ED_Admission, a dummy which is 1 if they went into the ED\n",
    "hosp['admissions']['edregtime'] = pd.to_datetime(hosp['admissions']['edregtime'])\n",
    "hosp['admissions']['edouttime'] = pd.to_datetime(hosp['admissions']['edouttime'])\n",
    "hosp['admissions']['ED_LOS'] = hosp['admissions']['edouttime'] - hosp['admissions']['edregtime']\n",
    "hosp['admissions']['ED_LOS'] = hosp['admissions']['ED_LOS'].dt.total_seconds() / (24 * 60 * 60)\n",
    "hosp['admissions']['ED_Admission'] = hosp['admissions']['ED_LOS'].notna().astype(int)\n",
    "\n",
    "# Include the length between admission and edregtime for good measure \n",
    "hosp['admissions']['admittime'] = pd.to_datetime(hosp['admissions']['admittime'])\n",
    "hosp['admissions']['ED_gap'] = hosp['admissions']['edregtime'] - hosp['admissions']['admittime']\n",
    "hosp['admissions']['ED_gap'] = hosp['admissions']['ED_gap'].dt.total_seconds() / (24 * 60 * 60)\n",
    "\n",
    "# Export the created features\n",
    "admission_measures = hosp['admissions'][['hadm_id', 'ED_Admission']].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Need for surgery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Codes for surgery\n",
    "surgery_codes = ['CSURG', 'NSURG', 'ORTHO', 'SURG', 'TSURG', 'VSURG', 'PSURG', 'TRAUM', 'OBS']\n",
    "\n",
    "# Create a dummy\n",
    "hosp['services']['Surgery_Admission'] = hosp['services']['curr_service'].apply(lambda x: x in surgery_codes).astype(int)\n",
    "\n",
    "# Export out\n",
    "surgery_out = hosp['services'][['hadm_id', 'Surgery_Admission']].copy()\n",
    "\n",
    "# Merge duplicate rows if the hospitalisation has at least 1 surgery\n",
    "surgery_out = surgery_out.groupby('hadm_id')['Surgery_Admission'].any().astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obstetrical Patients\n",
    "Obstetrical_hadm_ids = hosp['services'][hosp['services'].curr_service == 'OBS'].copy()\n",
    "\n",
    "# List of hospitalisations with OBS\n",
    "Obstetrical_hadm_ids = Obstetrical_hadm_ids.hadm_id.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Code status \n",
    "Code people as the most medically severe code that they have. e.g. If they are DNR at any point then code them as DNR only. Don't distinguish between partial code / DNR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select rows where 'itemid' is in code_status_ids.itemid. There are: 223758, 228687\n",
    "code_status_events = pd.read_csv(\"data/queried_data/codestatus.csv\")\n",
    "\n",
    "# Our grouping of full code, partial code and DNR\n",
    "full_code = [\"Full code\"]\n",
    "DNR = [\"DNAR (Do Not Attempt Resuscitation)  [DNR]\", \"DNR (do not resuscitate)\", \"DNR / DNI\", \"Comfort measures only\", \"DNAR (Do Not Attempt Resuscitation) [DNR] / DNI\", \"DNI (do not intubate)\"]\n",
    "\n",
    "# Add new columns. NOTE: need value here not valuenum (null)\n",
    "code_status_events[\"Full_code\"] = code_status_events.value.apply(lambda x: x in full_code).astype(int)\n",
    "code_status_events[\"DNR\"] = code_status_events.value.apply(lambda x: x in DNR).astype(int)\n",
    "\n",
    "# Reduce columns and perform the aggregation\n",
    "code_details = code_status_events[['stay_id', 'Full_code', 'DNR']].groupby('stay_id').any().astype(int)\n",
    "\n",
    "# Remove full_code from those with DNRs.\n",
    "code_details.loc[code_details['DNR'] ==1, 'Full_code'] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Severity of illness at hospital sdmission (LAPS II)\n",
    "See later [calculation of LAPS II requires features we create later on]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Predicted hospital mortality at hospital admission\n",
    "See later [calculation of LAPS II requires features we create later on]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Severity of illness on ICU admission\n",
    "\n",
    "Using SAPS II instead of SAPS III. The code to create the SAPS II data can be found [here](https://github.com/MIT-LCP/mimic-code/blob/main/mimic-iv/concepts/score/sapsii.sql). You can also download the derived data directly from GoogleBigQuery."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import SAPS II data\n",
    "sapsii = pd.read_csv(\"data/queried_data/sapsii.csv\").sort_values('stay_id')\n",
    "sapsii = sapsii[['stay_id', 'sapsii']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Days receiving benzodiazepines\n",
    "NOTE: can be slow to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A list of all benzodiazepines used in the MIMIC-IV, ICU data\n",
    "benzos = ['Diazepam (Valium)', 'Lorazepam (Ativan)', 'Midazolam (Versed)']\n",
    "\n",
    "# Associated itemids\n",
    "benzos_itemids = icu['d_items'][icu['d_items'].label.isin(benzos)].itemid\n",
    "\n",
    "# Consider just the treatments table.\n",
    "benzos_table = icu['inputevents'][icu['inputevents'].itemid.isin(benzos_itemids)].copy()\n",
    "\n",
    "# Generate 'date' columns by extracting the date part from starttime and endtime\n",
    "benzos_table['startdate'] = benzos_table['starttime'].dt.date\n",
    "benzos_table['enddate'] = benzos_table['endtime'].dt.date\n",
    "\n",
    "# Generate 'date' range column for each row\n",
    "benzos_table['date'] = benzos_table.apply(lambda row: pd.date_range(start=row['startdate'], end=row['enddate']).date.tolist(), axis=1)\n",
    "\n",
    "# Drop unnecessary columns to save memory. This is quite a neat idea\n",
    "benzos_table = benzos_table.drop(['starttime', 'endtime', 'startdate', 'enddate'], axis=1)\n",
    "\n",
    "# Flatten the 'date' column, remove duplicates within each 'stay_id' group, and reset index\n",
    "df_grouped = (benzos_table.explode('date')\n",
    "                        .groupby('stay_id')\n",
    "                        .date\n",
    "                        .apply(lambda x: x.drop_duplicates().tolist())\n",
    "                        .reset_index())\n",
    "\n",
    "# Calculate the number of unique dates where benzos were given\n",
    "benzos_info = df_grouped.copy()\n",
    "benzos_info['benzos_days'] = benzos_info['date'].str.len()\n",
    "benzos_info = benzos_info[['stay_id', 'benzos_days']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Days receiving other sedatives / non-benzodiazepines\n",
    "NOTE: can be slow to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A list of all non-benzodiazepines and other sedatives used (not including opioids). Removed all missing ones! \n",
    "other_seds = ['Propofol', 'Dexmedetomidine (Precedex)', 'Pentobarbital',\n",
    "'Ketamine', 'Haloperidol (Haldol)']\n",
    "\n",
    "# Associated itemids\n",
    "seds_itemids = icu['d_items'][icu['d_items'].label.isin(other_seds)].itemid\n",
    "\n",
    "# Consider the table with just seds shown\n",
    "seds_table1 = icu['procedureevents'][icu['procedureevents'].itemid.isin(seds_itemids)].copy() # Current nothing \n",
    "seds_table2 = icu['inputevents'][icu['inputevents'].itemid.isin(seds_itemids)].copy()\n",
    "seds_table3 = icu['ingredientevents'][icu['ingredientevents'].itemid.isin(seds_itemids)].copy() # Current nothing \n",
    "\n",
    "# Merge them for good practice.\n",
    "seds_table = pd.concat([seds_table1, seds_table2, seds_table3], axis = 0)\n",
    "\n",
    "# Generate 'date' columns by extracting the date part from starttime and endtime\n",
    "seds_table['startdate'] = seds_table['starttime'].dt.date\n",
    "seds_table['enddate'] = seds_table['endtime'].dt.date\n",
    "\n",
    "# Generate 'date' range column for each row. This is a very slow line\n",
    "seds_table['date'] = seds_table.apply(lambda row: pd.date_range(start=row['startdate'], end=row['enddate']).date.tolist(), axis=1)\n",
    "\n",
    "# Drop unnecessary columns to save memory\n",
    "seds_table = seds_table.drop(['starttime', 'endtime', 'startdate', 'enddate'], axis=1)\n",
    "\n",
    "# Flatten the 'date' column, remove duplicates within each 'stay_id' group, and reset index\n",
    "df_grouped_seds = (seds_table.explode('date')\n",
    "                                .groupby('stay_id')\n",
    "                                .date\n",
    "                                .apply(lambda x: x.drop_duplicates().tolist())\n",
    "                                .reset_index())\n",
    "\n",
    "# Calculate the number of unique dates where seds were given\n",
    "seds_info = df_grouped_seds.copy()\n",
    "seds_info['seds_days'] = seds_info['date'].str.len()\n",
    "seds_info = seds_info[['stay_id', 'seds_days']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Days receiving opiates\n",
    "NOTE: can be slow to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A list of all opiates used\n",
    "opiates = ['Fentanyl (Concentrate)', 'Morphine Sulfate', 'Meperidine (Demerol)', 'Fentanyl (Push)', 'Hydromorphone (Dilaudid)',\n",
    "'Methadone Hydrochloride', 'Fentanyl']\n",
    "\n",
    "# Associated itemids\n",
    "opiates_itemids = icu['d_items'][icu['d_items'].label.isin(opiates)].itemid\n",
    "\n",
    "# Consider just the inputevents table. All items are in the inputevents\n",
    "opiates_table = icu['inputevents'][icu['inputevents'].itemid.isin(opiates_itemids)].copy()\n",
    "\n",
    "# Generate 'date' columns by extracting the date part from starttime and endtime\n",
    "opiates_table['startdate'] = opiates_table['starttime'].dt.date\n",
    "opiates_table['enddate'] = opiates_table['endtime'].dt.date\n",
    "\n",
    "# Generate 'date' range column for each row\n",
    "opiates_table['date'] = opiates_table.apply(lambda row: pd.date_range(start=row['startdate'], end=row['enddate']).date.tolist(), axis=1)\n",
    "\n",
    "# Drop unnecessary columns to save memory\n",
    "opiates_table = opiates_table.drop(['starttime', 'endtime', 'startdate', 'enddate'], axis=1)\n",
    "\n",
    "# Flatten the 'date' column, remove duplicates within each 'stay_id' group, and reset index\n",
    "df_grouped_opiates = (opiates_table.explode('date')\n",
    "                                .groupby('stay_id')\n",
    "                                .date\n",
    "                                .apply(lambda x: x.drop_duplicates().tolist())\n",
    "                                .reset_index())\n",
    "\n",
    "# Calculate the number of unique dates where opiates were given\n",
    "opiates_info = df_grouped_opiates.copy()\n",
    "opiates_info['opiate_days'] = opiates_info['date'].str.len()\n",
    "opiates_info = opiates_info[['stay_id', 'opiate_days']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Hospital length of stay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hospital LOS\n",
    "hosp['admissions']['admittime'] = pd.to_datetime(hosp['admissions']['admittime'])\n",
    "hosp['admissions']['dischtime'] = pd.to_datetime(hosp['admissions']['dischtime'])\n",
    "hosp['admissions']['LOS'] = hosp['admissions']['dischtime'] - hosp['admissions']['admittime']\n",
    "hosp['admissions']['hosp_LOS'] = hosp['admissions']['LOS'].dt.total_seconds() / (24 * 60 * 60)\n",
    "hosp_los = hosp['admissions'][['hadm_id', 'subject_id', 'hosp_LOS']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Hospital mortality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vital_status_discharge = hosp['admissions'] [['hadm_id', 'subject_id', 'hospital_expire_flag']]\n",
    "vital_status_discharge = vital_status_discharge.rename(columns = {'hospital_expire_flag' : 'vital_status_discharge'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Discharge location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a discharged home column to the original DataFrame\n",
    "hosp['admissions']['discharged_home'] = hosp['admissions']['discharge_location'].apply(lambda x: (x==\"HOME\") | (x == \"HOME HEALTH CARE\") | (x==\"AGAINST ADVICE\") | (x==\"ASSISTED LIVING\")).astype(int)\n",
    "\n",
    "# Add a discharged to hospice column to the original DataFrame\n",
    "hosp['admissions']['discharged_hospice'] = hosp['admissions']['discharge_location'].apply(lambda x: x==\"HOSPICE\").astype(int)\n",
    "\n",
    "# Add a discharged to skilled nursing facility (SNF) column to the original DataFrame\n",
    "options = [\"SKILLED NURSING FACILITY\", 'REHAB', 'CHRONIC/LONG TERM ACUTE CARE', 'PSYCH FACILITY', 'ACUTE HOSPITAL', 'OTHER FACILITY', 'HEALTHCARE FACILITY']\n",
    "hosp['admissions']['discharged_skilled_facility'] = hosp['admissions']['discharge_location'].apply(lambda x: x in options).astype(int)\n",
    "\n",
    "# Then select the desired columns to form your new DataFrame.\n",
    "discharge_locations = hosp['admissions'][['hadm_id', 'discharge_location', 'discharged_home', 'discharged_hospice', 'discharged_skilled_facility']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Death within 30 days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Admission time\n",
    "hosp['admissions']['admittime'] = pd.to_datetime(hosp['admissions']['admittime'])\n",
    "hosp['admissions']\n",
    "\n",
    "# Date of death for each subject\n",
    "df_deaths = hosp['patients'][['subject_id', 'dod']]\n",
    "\n",
    "# New tables of subject id, hospitalisation id, admission times and deaths\n",
    "df_details = hosp['admissions'][['subject_id', 'hadm_id', 'admittime', 'dischtime']]\n",
    "\n",
    "# Combine this into one df\n",
    "deathin30_df = df_details.merge(df_deaths, left_on = \"subject_id\", right_on = \"subject_id\")\n",
    "\n",
    "# Set as a ts and edit the dates so that each date is midday\n",
    "deathin30_df['dod'] = pd.to_datetime(deathin30_df['dod'])\n",
    "deathin30_df['dod'] = deathin30_df['dod'].dt.normalize() + pd.Timedelta(hours=12)\n",
    "\n",
    "# Create a new colums if they died within 30 days of admission\n",
    "deathin30_df['time2death'] = deathin30_df['dod'] - deathin30_df['admittime']\n",
    "deathin30_df['deathin30'] = deathin30_df['time2death'].apply(lambda x: x < pd.Timedelta(days=30) if pd.notnull(x) else False).astype(int)\n",
    "\n",
    "# Sort by subject_id and then admittime\n",
    "deathin30_df.sort_values(['subject_id', 'admittime'], inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16. Hospital readmission within 30 days\n",
    "This should be calculated for each hospitalisation. First, get a list of all hospitalisation admissions. For each hospitalisation find out if there was a subsequent admission and, if so, when it was. Then calculate the time between discharge and subsequent admission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hosp['admissions'] = hosp['admissions'].sort_values(['subject_id', 'admittime'])\n",
    "hosp['admissions']['next_admission_date'] = hosp['admissions'].groupby('subject_id')['admittime'].shift(-1)\n",
    "\n",
    "# Create a new colums for readmission\n",
    "hosp['admissions']['readmission_time'] = hosp['admissions']['next_admission_date'] - hosp['admissions']['dischtime']\n",
    "hosp['admissions']['readmissionin30'] = hosp['admissions']['readmission_time'].apply(lambda x: x < pd.Timedelta(days=30) if pd.notnull(x) else False).astype(int)\n",
    "\n",
    "# Boil it down to relevant columns\n",
    "readmission_df = hosp['admissions'][['subject_id', 'hadm_id', 'admittime', 'dischtime', 'readmission_time', 'readmissionin30']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [6. From earlier] Severity of illness at hospital admission\n",
    "The LAPS2 Score. NOTE: LASP2 took a lot of calculation and is therefore toggled. The code is roughly a similar length to the rest of the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating LAPS2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We recode all of the datasets so that the stay_ids are mapped to the first stay_id of that hospitalisation. This way all of the icu stays in each hospitalisation are viewed as a single hospitalisatoin and we can base it around hospitalisations not ICU stays. This will help reframe the problem in terms of hospitalisations and make sure the data is as close to hospital admission as possible.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the Glasgow Coma Scale (gcs) data. This is from the derived section of the GoogleBigQuery database. NOTE: it is not the same as first_day_gcs\n",
    "gcs = pd.read_csv(\"data/queried_data/gcs.csv\")\n",
    "\n",
    "# Create a feature for gender\n",
    "gender = hosp['patients'][['subject_id', 'gender']].copy()\n",
    "gender['gender'] = (gender['gender'] == 'M').astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy of icu stays as about to perturb it\n",
    "icustays_individuals = icu['icustays'][['subject_id', 'hadm_id', 'stay_id']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a base icu dataset\n",
    "base = icu['icustays'][['hadm_id', 'stay_id', 'intime']].copy()\n",
    "\n",
    "# Define intime as a datatime\n",
    "base['intime'] = pd.to_datetime(base['intime'])\n",
    "\n",
    "# Sort by hadm_id and then by intime so that the first admissions are on top\n",
    "base.sort_values(['hadm_id', 'intime'], inplace = True)\n",
    "\n",
    "# Remove duplicates and keep the first to get the mapping. \n",
    "base_short = base.drop_duplicates(subset='hadm_id', keep='first').copy()\n",
    "\n",
    "# Create a mapping between hospitalions and stay_id (this is the first stay)\n",
    "first_icu_mapping = base_short.set_index('hadm_id')['stay_id'].to_dict()\n",
    "\n",
    "# Go back to base and add a new column which is the first stay_id\n",
    "base['first_icu'] = base['hadm_id'].apply(lambda x: first_icu_mapping[x])\n",
    "\n",
    "# Create another dict of the final mapping.\n",
    "stay_id_map = base.set_index('hadm_id')['first_icu'].to_dict()\n",
    "\n",
    "# Create final mapping (for datasets without hosp)\n",
    "original_stay_id2first_id = base.set_index('stay_id')['first_icu'].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dfs to transform (with and without hosp)\n",
    "dataframes_to_transform_1 = [icustays_individuals, icu['ingredientevents'], icu['inputevents'], icu['procedureevents']]\n",
    "dataframes_to_transform_2 = [icu['chartevents'], gcs]\n",
    "\n",
    "# Apply the mapping\n",
    "for df in dataframes_to_transform_1:\n",
    "    df['stay_id'] = df['hadm_id'].map(stay_id_map)\n",
    "\n",
    "for df in dataframes_to_transform_2:\n",
    "    df['stay_id'] = df['stay_id'].map(original_stay_id2first_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cut icustays down. This avoid duplicates being added.\n",
    "icustays_individuals = icustays_individuals.drop_duplicates(keep='first').copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define functions to collect the first reading of each hospitalisation. Need different collectors for different datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### A function to collect the first reading of each hospitalisation\n",
    "def hosp_lab_collector(labels, details = False):\n",
    "\n",
    "    # Return ids\n",
    "    labels_ids = hosp['d_labitems'][hosp['d_labitems'].label.isin(labels)].itemid\n",
    "    if details == True:\n",
    "        print(hosp['d_labitems'][hosp['d_labitems'].label.isin(labels)])\n",
    "\n",
    "    # Make smaller table\n",
    "    table = hosp['labevents'][hosp['labevents'].itemid.isin(labels_ids)].copy()\n",
    "    if details == True:\n",
    "        print(table.itemid.value_counts())\n",
    "\n",
    "    # Record '___' as pd.NA\n",
    "    #display(table)\n",
    "    table = table.replace('___', pd.NA)\n",
    "\n",
    "    # Create an auxiliary column\n",
    "    table['is_value_na'] = table['valuenum'].isnull()\n",
    "\n",
    "    # Sort by hadm_id, then by whether the value is null, and then by charttime. This is neats. Avoids NAs unless it has to!\n",
    "    table.sort_values(['hadm_id', 'is_value_na', 'charttime'], inplace = True)\n",
    "\n",
    "    # Remove duplicates of hadm_id (keep the first)\n",
    "    table = table.drop_duplicates(subset='hadm_id', keep='first')\n",
    "\n",
    "    return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### A function to collect the first reading of each hospitalisation\n",
    "def icu_chart_collector_secondary(labels, details = False, item_ids = []):\n",
    "    \"\"\"\n",
    "    The original ICU collector. For use in the prelim round and in the secondary round if appropriate\n",
    "    This works across secondary (uses the secondary dataset)\n",
    "    \"\"\"\n",
    "\n",
    "    # Return ids\n",
    "    labels_ids = icu['d_items'][icu['d_items'].label.isin(labels)].itemid\n",
    "\n",
    "    if len(item_ids) > 0:\n",
    "        labels_ids = item_ids\n",
    "\n",
    "    if details == True:\n",
    "        display(icu['d_items'][icu['d_items'].label.isin(labels)])\n",
    "\n",
    "    # Make smaller table from icu['chartevents']\n",
    "    table_icu = icu['chartevents'][icu['chartevents'].itemid.isin(labels_ids)].copy()\n",
    "    if details == True:\n",
    "        display(table_icu)\n",
    "        display(table_icu.itemid.value_counts())\n",
    "\n",
    "    # Switch relevant values to NA\n",
    "    table_icu = table_icu.replace('___', pd.NA)\n",
    "\n",
    "    # Create an auxiliary column\n",
    "    table_icu['is_value_na'] = table_icu['valuenum'].isnull()\n",
    "\n",
    "    # Sort by stay_id, then by whether the value is null, and then by charttime. Avoids NAs unless it has to!\n",
    "    table_icu.sort_values(['stay_id', 'is_value_na', 'charttime'], inplace = True)\n",
    "\n",
    "    # Remove duplicates of stay_id (keep the first)\n",
    "    table_icu = table_icu.drop_duplicates(subset='stay_id', keep='first')\n",
    "\n",
    "    return table_icu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the secondary model's feature collector. Note that I use the prelim model ICU collector.\n",
    "def hosp_lab_collector_secondary(labels, worse, details = False, item_ids = []):\n",
    "    \"\"\"\n",
    "    Added a time filter. Only includes those within 72 hours of hospital admission. \n",
    "    If multiple values returns the most medically severe value. Min / Max needs to be defined\n",
    "    \"\"\"\n",
    "\n",
    "    # Return ids\n",
    "    labels_ids = hosp['d_labitems'][hosp['d_labitems'].label.isin(labels)].itemid\n",
    "\n",
    "    if len(item_ids) > 0:\n",
    "        labels_ids = item_ids\n",
    "\n",
    "    if details == True:\n",
    "        print(hosp['d_labitems'][hosp['d_labitems'].label.isin(labels)])\n",
    "\n",
    "    # Make smaller table from secondary_labevents\n",
    "    table = hosp['labevents'][hosp['labevents'].itemid.isin(labels_ids)].copy()\n",
    "    if details == True:\n",
    "        print(table.itemid.value_counts())\n",
    "\n",
    "    #### TIME FILTER ####\n",
    "\n",
    "    # TIME FILTER: Append hospital admission time. \n",
    "    table = table.merge(hosp['admissions'][['hadm_id', 'admittime']], on=\"hadm_id\", how='left')\n",
    "\n",
    "    # TIME FILTER: Make things date time\n",
    "    table['admittime'] = pd.to_datetime(table['admittime'])\n",
    "    table['charttime'] = pd.to_datetime(table['charttime'])\n",
    "\n",
    "    # TIME FILTER: Make a time after admission col and filter to only those in 72 hours of admission\n",
    "    table['time_after_admission'] = table['charttime'] - table['admittime']\n",
    "    table  =  table[table['time_after_admission'] < pd.Timedelta(hours=72)]\n",
    "\n",
    "    #### DEAL WITH MULTIPLE VALUES ####\n",
    "\n",
    "    # Record '___' as pd.NA\n",
    "    table = table.replace('___', pd.NA)\n",
    "    \n",
    "    # Create an auxiliary column\n",
    "    table['is_value_na'] = table['valuenum'].isnull()\n",
    "\n",
    "    if worse == 'higher':\n",
    "        table.sort_values(by=['hadm_id', 'is_value_na', 'valuenum'], ascending=[True, True, False], inplace=True)\n",
    "\n",
    "    if worse == 'lower':\n",
    "        table.sort_values(by=['hadm_id', 'is_value_na', 'valuenum'], ascending=[True, True, True], inplace=True)\n",
    "\n",
    "    # Remove duplicates of hadm_id (keep the first)\n",
    "    table = table.drop_duplicates(subset='hadm_id', keep='first')\n",
    "\n",
    "    return table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preliminary model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Collect BUN Manually #### itemid = 225624\n",
    "\n",
    "# Define the labels. Removed ['BUN_ApacheIV', 'BunScore_ApacheIV'] as scores seem a little different. Only 8 of 227001 and 227000 (the rest are BUN)\n",
    "bun_icu = ['BUN']\n",
    "\n",
    "# Table of items that suffice as BUN\n",
    "bun_icu_itemids = icu['d_items'][icu['d_items'].label.isin(bun_icu)].itemid\n",
    "\n",
    "# Chartevents filtered by BUN. \n",
    "chartevents_bun = icu['chartevents'][icu['chartevents'].itemid.isin(bun_icu_itemids)].copy()\n",
    "\n",
    "# Make charttime a time\n",
    "chartevents_bun['charttime'] = pd.to_datetime(chartevents_bun['charttime'])\n",
    "\n",
    "# Record '___' as pd.NA\n",
    "chartevents_bun = chartevents_bun.replace('___', pd.NA)\n",
    "\n",
    "# Create an auxiliary column\n",
    "chartevents_bun['is_value_na'] = chartevents_bun['valuenum'].isnull()\n",
    "\n",
    "# Sort by stay_id and then charttime\n",
    "chartevents_bun.sort_values(['stay_id', 'is_value_na', 'charttime'], inplace = True)\n",
    "\n",
    "# Remove duplicates of stay_id (keep the first)\n",
    "chartevents_bun = chartevents_bun.drop_duplicates(subset='stay_id', keep='first')\n",
    "\n",
    "# Rename and export\n",
    "chartevents_bun.rename(columns = {'valuenum' : 'BUN'}, inplace = True)\n",
    "chartevents_bun = chartevents_bun[['stay_id', 'BUN']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creatinine details 50912, 51081, 51977, 52024, 52546\n",
    "Creatinine_hosp = ['Creatinine', 'Creatinine, Whole Blood', 'Creatinine, Serum', 'Creatinine, Blood']\n",
    "Creatinine_labevents = hosp_lab_collector(Creatinine_hosp, details = False)[['hadm_id', 'valuenum']]\n",
    "Creatinine_labevents.rename(columns = {'valuenum':'creatinine'}, inplace = True)\n",
    "\n",
    "#### SODIUM #### 50983 (Sodium or Sodium [Moles/volume] in Serum or Plasma). 50824, 50983, 52455, 52623\n",
    "#  50824 (Sodium, Whole Blood). All measured in mEq/L\n",
    "Sodium_hosp = ['Sodium', 'Sodium, Whole Blood']\n",
    "Sodium_labevents = hosp_lab_collector(Sodium_hosp, details = False)[['hadm_id', 'valuenum']]\n",
    "Sodium_labevents.rename(columns = {'valuenum':'sodium'}, inplace = True)\n",
    "\n",
    "#### ANION GAP #### 52500 and 50868 (both Anion gap in Serum or Plasma). Both mEq/L. All 50868 (Anion gap 4 in Serum or Plasma). 50868, 52500\n",
    "ANION_hosp = ['Anion Gap']\n",
    "ANION_labevents = hosp_lab_collector(ANION_hosp, details = False)[['hadm_id', 'valuenum']]\n",
    "ANION_labevents.rename(columns = {'valuenum':'anion_gap'}, inplace = True)\n",
    "\n",
    "#### Serum Bicarbonate #### Almsot all 50882 (mEq/L) (Bicarbonate [Moles/volume] in Serum or Plasma). 50803, 50882, 52039\n",
    "Bicarbonate_hosp = ['Bicarbonate', 'Calculated Bicarbonate, Whole Blood', 'Calculated Bicarbonate']\n",
    "Bicarbonate_labevents = hosp_lab_collector(Bicarbonate_hosp, details = False)[['hadm_id', 'valuenum']]\n",
    "Bicarbonate_labevents.rename(columns = {'valuenum':'Bicarbonate'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Prelim Model Dataset ###\n",
    "stays = icustays_individuals[['subject_id', 'hadm_id', 'stay_id']]\n",
    "stays = stays.merge(Creatinine_labevents, on=\"hadm_id\", how='left')\n",
    "stays = stays.merge(chartevents_bun, on=\"stay_id\", how='left')\n",
    "stays = stays.merge(Bicarbonate_labevents, on=\"hadm_id\", how='left')\n",
    "stays = stays.merge(ANION_labevents, on=\"hadm_id\", how='left')\n",
    "stays = stays.merge(Sodium_labevents, on=\"hadm_id\", how='left')\n",
    "stays = stays.merge(derived_age, on=\"hadm_id\", how='left')\n",
    "stays = stays.merge(gender, on=\"subject_id\", how='left')\n",
    "stays = stays.merge(admission_measures[['hadm_id', 'ED_Admission']], on=\"hadm_id\", how='left') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make floats\n",
    "for col in ['creatinine', 'BUN', 'Bicarbonate', 'anion_gap','sodium']:\n",
    "    stays[col] = pd.to_numeric(stays[col], errors = 'coerce')\n",
    "\n",
    "# Remove extreme outliers. Define upper limits for each lab value\n",
    "upper_limits = {\n",
    "    'creatinine': 3,  \n",
    "    'BUN': 100.0,  \n",
    "    'Bicarbonate': 50.0,  \n",
    "    'anion_gap': 30.0, \n",
    "    'sodium': 200.0\n",
    "}\n",
    "\n",
    "# Replace outliers with NA\n",
    "for col, upper_limit in upper_limits.items():\n",
    "    stays.loc[stays[col] > upper_limit, col] = np.nan\n",
    "\n",
    "# Define representative values.\n",
    "representative_values = {\n",
    "    'creatinine': 1.0,\n",
    "    'BUN': 14.0,\n",
    "    'Bicarbonate': 26.0,\n",
    "    'anion_gap': 7.0, \n",
    "    'sodium': 140.0\n",
    "}\n",
    "\n",
    "# Impute missing values\n",
    "for col, representative_value in representative_values.items():\n",
    "    stays[col].fillna(representative_value, inplace=True)\n",
    "\n",
    "# Make ratios\n",
    "stays['BUN/creatinine'] = stays['BUN'] / stays['creatinine']\n",
    "stays['AnionGap/SerumBicarbonate'] = (stays['anion_gap'] / stays['Bicarbonate']) *1000\n",
    "\n",
    "### Make all of the dummies ###\n",
    "\n",
    "def create_dummies(df, column, bins):\n",
    "    # Create binned categories\n",
    "    categories = pd.cut(df[column], bins, labels=False, include_lowest=True, right=False)\n",
    "\n",
    "    # Create dummies\n",
    "    dummies = pd.get_dummies(categories, prefix=column)\n",
    "\n",
    "    # Drop the original column from the DataFrame and add the dummies\n",
    "    df = pd.concat([df, dummies], axis=1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Define the bins. Note that if you are on the edge you move into the upper bin.\n",
    "age_bins = [18, 39, 64, 74, 84, float('inf')]  # 'inf' for ages 85 and above. Reference 18 - 39 (age_0)\n",
    "BUN_creatinine_bins = [0, 8, 16, 24, float('inf')] # Reference 8 - 16 BUN/creatinine_1\n",
    "sodium_bins = [0, 129, 132, 135, 146, 149, 155, float('inf')] # Reference 135 â€“ 146, sodium_3\n",
    "anion_gap_bicarbonate_bins = [0, 200, 400, 600, float('inf')] # Reference 200 - 400, AnionGap/SerumBicarbonate_1\n",
    "\n",
    "# Apply the function\n",
    "stays = create_dummies(stays, 'age', age_bins)\n",
    "stays = create_dummies(stays, 'BUN/creatinine', BUN_creatinine_bins)\n",
    "stays = create_dummies(stays, 'sodium', sodium_bins)\n",
    "stays = create_dummies(stays, 'AnionGap/SerumBicarbonate', anion_gap_bicarbonate_bins)\n",
    "\n",
    "stays_original = stays.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only keep necessary columns\n",
    "stays_trim = stays[['stay_id', 'age_1', 'age_2', 'age_3', 'age_4', 'gender', 'ED_Admission',\n",
    "  'BUN/creatinine_0.0', 'BUN/creatinine_2.0', 'BUN/creatinine_3.0', 'sodium_0', 'sodium_1', 'sodium_2'\n",
    " , 'sodium_4', 'sodium_5', 'sodium_6', 'AnionGap/SerumBicarbonate_0.0', 'AnionGap/SerumBicarbonate_2.0',\n",
    " 'AnionGap/SerumBicarbonate_3.0']].copy()\n",
    "\n",
    " # Make the coefficient dictionary\n",
    "coeffs = {\n",
    "    'age_1': -0.25234, \n",
    "    'age_2': 0.25894, \n",
    "    'age_3': 0.48826, \n",
    "    'age_4': 0.87647, \n",
    "    'gender': 0.27430, \n",
    "    'ED_Admission': 1.39670,\n",
    "    'BUN/creatinine_0.0': 0.26988, \n",
    "    'BUN/creatinine_2.0': -0.22465, \n",
    "    'BUN/creatinine_3.0': 0.39858, \n",
    "    'sodium_0': 0.11980, \n",
    "    'sodium_1': -0.06801, \n",
    "    'sodium_2': -0.30494,\n",
    "    'sodium_4': -0.02560, \n",
    "    'sodium_5': 0.42071, \n",
    "    'sodium_6': 0.58891, \n",
    "    'AnionGap/SerumBicarbonate_0.0': -0.20038, \n",
    "    'AnionGap/SerumBicarbonate_2.0': -0.11174,\n",
    "    'AnionGap/SerumBicarbonate_3.0': 0.70227\n",
    "}\n",
    "\n",
    "intercept = -4.31678"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features and coefficients in the same order\n",
    "features = list(coeffs.keys())\n",
    "coefficients = np.array(list(coeffs.values()))\n",
    "\n",
    "# Apply the logistic regression formula\n",
    "linear_combination = np.dot(stays_trim[features], coefficients) + intercept\n",
    "l = [-i for i in linear_combination] # Overcome weird bug. Confirmed that this gets the correct output\n",
    "probabilities = 1 / (1 + np.exp(l))\n",
    "\n",
    "# Add the probs to the stays original\n",
    "stays_original['probs'] = probabilities\n",
    "\n",
    "# Have a columns indicating whether they are high-risk of not (i.e. prob > 0.06)\n",
    "stays_original['high_risk'] = (stays_original['probs']>0.06).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Secondary model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Codes needed\n",
    "\n",
    "# 50820, 52041, 50831, 51094, 51491, 52730, 50813, 52442, 53154, 50824, 50983, 52455, 52623, 50885, 50912, 51081, 51977, 52024, 52546\n",
    "# 50862, 51775, 51776, 51807, 51836, 51910, 51926, 52022, 53085, 53116, 53138, 50809, 50931, 51478, 51981, 52027, 52569, 51221, 51480\n",
    "# 51638, 51639, 52028, 51300, 51301, 51755, 51756, 53134, 50818, 52040, 50821, 52042, 51003, 50825\n",
    "\n",
    "# 225624, 227000, 227001, 228152, 229669, 220227, 226767, 226860, 226861, 226862, 226863, 226865, 227035, 228232, 220179, 225309, 226850\n",
    "# 223761, 223762, 220045, 220210, 224688, 224690, 224745, 225475, 227032, 227047, 229425, 229563, 220050, 220052, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Arterial pH ####\n",
    "Arterial_pH = ['pH', 'pH ', 'pH  ']\n",
    "Arterial_pH_labevents = hosp_lab_collector_secondary(Arterial_pH, worse = 'lower', details = False, item_ids = [50820, 52041])[['hadm_id', 'valuenum']]\n",
    "Arterial_pH_labevents.rename(columns = {'valuenum':'Arterial_pH'}, inplace = True)\n",
    "\n",
    "#### Lactate #### \n",
    "Lactate_hosp = ['Lactate'] # Just want this. Measrued in mmol/L so okay.\n",
    "Lactate_labevents = hosp_lab_collector_secondary(Lactate_hosp, worse = 'higher', details = False)[['hadm_id', 'valuenum']]\n",
    "Lactate_labevents.rename(columns = {'valuenum':'Lactate'}, inplace = True)\n",
    "\n",
    "#### SODIUM #### \n",
    "#  50983 (Sodium or Sodium [Moles/volume] in Serum or Plasma). Same unit as mEq/L for sodium\n",
    "#  50824 (Sodium, Whole Blood). All measured in mEq/L\n",
    "Sodium_hosp = ['Sodium', 'Sodium, Whole Blood']\n",
    "Sodium_labevents = hosp_lab_collector_secondary(Sodium_hosp, worse = 'lower', details = False)[['hadm_id', 'valuenum']]\n",
    "Sodium_labevents.rename(columns = {'valuenum':'sodium'}, inplace = True)\n",
    "\n",
    "#### TOTAL SERUM BILIRUBIN #### 50885\n",
    "BILIRUBIN_hosp = []\n",
    "BILIRUBIN_labevents = hosp_lab_collector_secondary(BILIRUBIN_hosp, worse = 'higher', details = False, item_ids=[50885])[['hadm_id', 'valuenum']]\n",
    "BILIRUBIN_labevents.rename(columns = {'valuenum':'Biblirubin'}, inplace = True)\n",
    "\n",
    "#### BUN #### \n",
    "BUN_icu = ['BUN', 'BUN_ApacheIV', 'BunScore_ApacheIV']\n",
    "BUN_events = icu_chart_collector_secondary(BUN_icu, details = False)[['stay_id', 'valuenum']]\n",
    "BUN_events.rename(columns = {'valuenum':'BUN'}, inplace = True)\n",
    "\n",
    "# Creatinine details\n",
    "Creatinine_hosp = ['Creatinine', 'Creatinine, Whole Blood', 'Creatinine, Serum', 'Creatinine, Blood']\n",
    "Creatinine_labevents = hosp_lab_collector_secondary(Creatinine_hosp, worse = 'higher', details = False, item_ids=[50912, 52024, 52546])[['hadm_id', 'valuenum']]\n",
    "Creatinine_labevents.rename(columns = {'valuenum':'creatinine'}, inplace = True)\n",
    "\n",
    "# Change 0s to NA (will get the same points in the scoring)\n",
    "Creatinine_labevents['creatinine'].replace(0.0, pd.NA, inplace = True)\n",
    "\n",
    "# Albumin details\n",
    "Albumin_hosp = ['Albumin, Blood', '(Albumin)', 'Albumin', '<Albumin>', '(Albumin) ', 'Albumin ']\n",
    "Albumin_labevents = hosp_lab_collector_secondary(Albumin_hosp, worse = 'lower', details = False, item_ids=[50862, 52022])[['hadm_id', 'valuenum']]\n",
    "Albumin_labevents.rename(columns = {'valuenum':'albumin'}, inplace = True)\n",
    "\n",
    "# Glucose details\n",
    "Glucose_hosp = ['Glucose, Whole Blood', 'Glucose', 'Glucose ']\n",
    "Glucose_labevents = hosp_lab_collector_secondary(Glucose_hosp, worse = 'lower', details = False, item_ids=[50931, 50809, 52027, 52569])[['hadm_id', 'valuenum']]\n",
    "Glucose_labevents.rename(columns = {'valuenum':'glucose'}, inplace = True)\n",
    "\n",
    "# Hematocrit details. All 51221\n",
    "Hematocrit_hosp = ['Hematocrit', 'Hematocrit ', 'Hematocrit  ']\n",
    "Hematocrit_labevents = hosp_lab_collector_secondary(Hematocrit_hosp, worse = 'lower', details = False, item_ids=[51221, 51638, 51639, 52028])[['hadm_id', 'valuenum']]\n",
    "Hematocrit_labevents.rename(columns = {'valuenum':'hematocrit'}, inplace = True)\n",
    "\n",
    "# WBC. 51301 or 51300 but all work\n",
    "white_blood_cells_hosp = ['White Blood Cells', 'Absolute Other WBC', 'WBC Count']\n",
    "white_blood_cells_labevents = hosp_lab_collector_secondary(white_blood_cells_hosp, worse = 'higher', details = False, item_ids=[])[['hadm_id', 'valuenum']]\n",
    "white_blood_cells_labevents.rename(columns = {'valuenum':'white_blood_cells'}, inplace = True)\n",
    "\n",
    "# ARTERIAL PaCO2 (mm Hg).\n",
    "pCO2_hosp = ['pCO2', 'pCO2 ']\n",
    "pCO2_labevents = hosp_lab_collector_secondary(pCO2_hosp, worse = 'higher', details = False, item_ids=[])[['hadm_id', 'valuenum']]\n",
    "pCO2_labevents.rename(columns = {'valuenum':'pCO2'}, inplace = True)\n",
    "\n",
    "# ARTERIAL PaO2 (mm Hg).\n",
    "pO2_hosp = ['pO2', 'pO2 ']\n",
    "pO2_labevents = hosp_lab_collector_secondary(pO2_hosp, worse = 'higher', details = False, item_ids=[])[['hadm_id', 'valuenum']]\n",
    "pO2_labevents.rename(columns = {'valuenum':'pO2'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Troponin T\n",
    "\n",
    "Troponin_labels = ['Troponin T']\n",
    "# Return ids\n",
    "Troponin_labels_ids = hosp['d_labitems'][hosp['d_labitems'].label.isin(Troponin_labels)].itemid\n",
    "\n",
    "# Make smaller table\n",
    "Troponin_table = hosp['labevents'][hosp['labevents'].itemid.isin(Troponin_labels_ids)].copy()\n",
    "\n",
    "#### TIME FILTER ####\n",
    "\n",
    "# TIME FILTER: Append hospital admission time. \n",
    "Troponin_table = Troponin_table.merge(hosp['admissions'][['hadm_id', 'admittime']], on=\"hadm_id\", how='left')\n",
    "\n",
    "# TIME FILTER: Make things date time\n",
    "Troponin_table['admittime'] = pd.to_datetime(Troponin_table['admittime'])\n",
    "Troponin_table['charttime'] = pd.to_datetime(Troponin_table['charttime'])\n",
    "\n",
    "# TIME FILTER: Make a time after admission col and filter to only those in 72 hours of admission\n",
    "Troponin_table['time_after_admission'] = Troponin_table['charttime'] - Troponin_table['admittime']\n",
    "Troponin_table  =  Troponin_table[Troponin_table['time_after_admission'] < pd.Timedelta(hours=72)]\n",
    "\n",
    "#### DEAL WITH MULTIPLE VALUES ####\n",
    "\n",
    "# Record '___' as pd.NA\n",
    "Troponin_table = Troponin_table.replace('___', pd.NA)\n",
    "\n",
    "# Create an auxiliary column\n",
    "Troponin_table['is_value_na'] = Troponin_table['valuenum'].isnull()\n",
    "\n",
    "Troponin_table.sort_values(by=['hadm_id', 'is_value_na', 'valuenum'], ascending=[True, True, False], inplace=True)\n",
    "\n",
    "# Remove duplicates of hadm_id (keep the first)\n",
    "Troponin_table = Troponin_table.drop_duplicates(subset='hadm_id', keep='first')\n",
    "\n",
    "Troponin_table = Troponin_table[['hadm_id', 'valuenum']]\n",
    "Troponin_table.rename(columns = {'valuenum':'Troponin_T'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temperature\n",
    "Temperature_hosp = ['Temperature']\n",
    "Temperature_labevents = hosp_lab_collector_secondary(Temperature_hosp, worse = 'lower', details = False, item_ids=[])[['hadm_id', 'valuenum']]\n",
    "Temperature_labevents.rename(columns = {'valuenum':'temperature'}, inplace = True)\n",
    "\n",
    "# Temperature (FROM ICU - preferred). Note that custom function used to convert C ---> F\n",
    "temp_labels = ['Temperature Fahrenheit', 'Temperature Celsius'] # 223761, 223762\n",
    "\n",
    "# Return ids\n",
    "temp_labels_ids = icu['d_items'][icu['d_items'].label.isin(temp_labels)].itemid\n",
    "\n",
    "# Make smaller table\n",
    "temp_table = icu['chartevents'][icu['chartevents'].itemid.isin(temp_labels_ids)].copy()\n",
    "\n",
    "# Switch relevant values to NA\n",
    "temp_table = temp_table.replace('___', pd.NA)\n",
    "\n",
    "# Convert C into F\n",
    "temp_table['valuenum'] = pd.to_numeric(temp_table['valuenum'], errors = 'coerce')\n",
    "temp_table['valuenum'] = temp_table.apply(lambda row: ((row['valuenum'] * 9/5) + 32) if row['itemid'] == 223762 else row['valuenum'], axis=1)\n",
    "\n",
    "# Create an auxiliary column\n",
    "temp_table['is_value_na'] = temp_table['valuenum'].isnull()\n",
    "\n",
    "# Sort by stay_id, then by whether the value is null, and then by charttime.\n",
    "temp_table.sort_values(['stay_id', 'is_value_na', 'charttime'], inplace = True)\n",
    "\n",
    "# Remove duplicates of stay_id (keep the first)\n",
    "temp_table = temp_table.drop_duplicates(subset='stay_id', keep='first')\n",
    "temp_table.rename(columns = {'valuenum':'temperature'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heart Rate (beats per minute) 220045 (Heart Rate)\n",
    "HR_hosp = ['Heart Rate']\n",
    "HR_labevents = icu_chart_collector_secondary(HR_hosp, details = False, item_ids=[])[['stay_id', 'valuenum']]\n",
    "HR_labevents.rename(columns = {'valuenum':'HR'}, inplace = True)\n",
    "\n",
    "# RR [ICU]. 220210 (Respiratory Rate), 224690 Respiratory Rate (Total), 224688\n",
    "RR_ICU = ['Plan-Respiratory', 'ROS-Respiratory', 'Post-Operative Respiratory (RESPIRAT)', 'Non-Operative Respiratory (RESPIRAT)',\n",
    "'Respiratory Rate (Set)', 'Respiratory Quotient', 'Respiratory Rate (Total)', 'Respiratory Rate',\n",
    " 'Respiratory Arrest']\n",
    "RR_ICU_EVENTS = icu_chart_collector_secondary(RR_ICU, details = False, item_ids=[220210, 224690, 224688])[['stay_id', 'valuenum']]\n",
    "RR_ICU_EVENTS.rename(columns = {'valuenum':'RR'}, inplace = True)\n",
    "\n",
    "# Systolic Blood Pressure 220179 (Non Invasive Blood Pressure systolic), 220052 (PA systolic pressure(PA Line))\n",
    "# 220050 (Arterial Blood Pressure systolic). 225309 (ART BP Systolic). 220059 (Pulmonary Artery Pressure systolic)\n",
    "Blood_pressure_ICU = ['Pulmonary Atrtery Pressure Signal - Systolic',\n",
    " 'Aortic Pressure Signal - Systolic',\n",
    " 'RV systolic pressure(PA Line)', 'ART BP Systolic',\n",
    " 'Non Invasive Blood Pressure systolic', 'Arterial Blood Pressure systolic', \n",
    "  'Arterial Blood Pressure mean']\n",
    "Blood_pressure = icu_chart_collector_secondary(Blood_pressure_ICU, details = False, item_ids=[220179, 220050, 225309])[['stay_id', 'valuenum']]\n",
    "Blood_pressure.rename(columns = {'valuenum':'blood_pressure'}, inplace = True)\n",
    "\n",
    "# Change 0s to 1s (will get the same points in the scoring)\n",
    "Blood_pressure['blood_pressure'].replace(0.0, 1, inplace = True)\n",
    "\n",
    "# Oxygend Saturation 220277 is SpO2 - this is the ideal. 220227 is SaO2 (a bit different).\n",
    "Oxygen_ICU = ['PAR-Oxygen saturation', \n",
    " 'OxygenApacheIIScore',\n",
    " 'OxygenScore_ApacheIV', 'Arterial O2 Saturation',\n",
    " 'O2 saturation pulseoxymetry', 'PA %O2 Saturation (PA Line)', \n",
    "  'PVR %O2 Saturation (PA Line)', 'ART %O2 saturation (PA Line)', 'SVR %O2 Saturation (PA Line)', 'RA %O2 Saturation (PA Line)']\n",
    "Oxygen_pressure = icu_chart_collector_secondary(Oxygen_ICU, details = False, item_ids=[220277, 220227])[['stay_id', 'valuenum']]\n",
    "Oxygen_pressure.rename(columns = {'valuenum':'oxygen'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neurological Score (GCS) ---> Separate Derived Table\n",
    "\n",
    "# Record '___' as pd.NA\n",
    "gcs_table = gcs.replace('___', pd.NA)\n",
    "\n",
    "# Create an auxiliary column\n",
    "gcs_table['is_value_na'] = gcs_table['gcs'].isnull()\n",
    "\n",
    "# Define charttime as a time\n",
    "gcs_table['charttime'] = pd.to_datetime(gcs_table['charttime'])\n",
    "gcs_table.sort_values(['stay_id', 'charttime'], inplace = True)\n",
    "\n",
    "# Remove duplicates of stay_id (keep the first)\n",
    "gcs_table = gcs_table.drop_duplicates(subset='stay_id', keep='first')\n",
    "\n",
    "# Dictionary of values\n",
    "gcs_dict = {15:\"Normal\", 14:\"Abmiguous\", 13:\"Abmiguous\",\n",
    "12:\"Abnormal\", 11:\"Abnormal\", 10:\"Abnormal\", 9:\"Abnormal\",\n",
    "8:\"Very Abnormal\", 7:\"Very Abnormal\", 6:\"Very Abnormal\", 5:\"Very Abnormal\",\n",
    "4:\"Very Abnormal\", 3:\"Very Abnormal\", 2:\"Very Abnormal\", 1:\"Very Abnormal\", 0:\"Very Abnormal\"}\n",
    "\n",
    "gcs_table['Neurological_Score'] = gcs_table[\"gcs\"].apply(lambda x: gcs_dict[x])\n",
    "gcs_table = gcs_table[[\"stay_id\", \"Neurological_Score\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Putting it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a base\n",
    "LAPS2_DF = icustays_individuals[['subject_id', 'hadm_id', 'stay_id']]\n",
    "\n",
    "# Merge in the high risk category \n",
    "LAPS2_DF = LAPS2_DF.merge(stays_original[['stay_id', 'high_risk']], on=\"stay_id\", how='left')\n",
    "\n",
    "# Merge in actual death for checks and for the model\n",
    "death = hosp['admissions'][['hospital_expire_flag', 'hadm_id']].copy()\n",
    "death.rename(columns = {'hospital_expire_flag':'death'}, inplace = True)\n",
    "LAPS2_DF = LAPS2_DF.merge(death, on=\"hadm_id\", how='left')\n",
    "\n",
    "# Merge in death within 30 days of admission deathin30_df\n",
    "deathin30 = deathin30_df[['hadm_id', 'deathin30']].copy()\n",
    "LAPS2_DF = LAPS2_DF.merge(deathin30, on=\"hadm_id\", how='left')\n",
    "\n",
    "# Merge everything in\n",
    "LAPS2_DF = LAPS2_DF.merge(Arterial_pH_labevents, on=\"hadm_id\", how='left')\n",
    "LAPS2_DF = LAPS2_DF.merge(Lactate_labevents, on=\"hadm_id\", how='left')\n",
    "LAPS2_DF = LAPS2_DF.merge(Sodium_labevents, on=\"hadm_id\", how='left')\n",
    "LAPS2_DF = LAPS2_DF.merge(BILIRUBIN_labevents, on=\"hadm_id\", how='left')\n",
    "LAPS2_DF = LAPS2_DF.merge(BUN_events, on=\"stay_id\", how='left')\n",
    "LAPS2_DF = LAPS2_DF.merge(Creatinine_labevents, on=\"hadm_id\", how='left')\n",
    "\n",
    "# Make BUN_creatinine\n",
    "LAPS2_DF['BUN_creatinine'] = LAPS2_DF['BUN'] / LAPS2_DF['creatinine']\n",
    "\n",
    "LAPS2_DF = LAPS2_DF.merge(Albumin_labevents, on=\"hadm_id\", how='left') \n",
    "LAPS2_DF = LAPS2_DF.merge(Glucose_labevents, on=\"hadm_id\", how='left') \n",
    "LAPS2_DF = LAPS2_DF.merge(Hematocrit_labevents, on=\"hadm_id\", how='left') \n",
    "LAPS2_DF = LAPS2_DF.merge(white_blood_cells_labevents, on=\"hadm_id\", how='left') \n",
    "LAPS2_DF = LAPS2_DF.merge(pCO2_labevents, on=\"hadm_id\", how='left') \n",
    "LAPS2_DF = LAPS2_DF.merge(pO2_labevents, on=\"hadm_id\", how='left') \n",
    "LAPS2_DF = LAPS2_DF.merge(Troponin_table, on=\"hadm_id\", how='left') \n",
    "LAPS2_DF = LAPS2_DF.merge(temp_table[['stay_id', 'temperature']], on=\"stay_id\", how='left') \n",
    "LAPS2_DF = LAPS2_DF.merge(HR_labevents, on=\"stay_id\", how='left') \n",
    "LAPS2_DF = LAPS2_DF.merge(RR_ICU_EVENTS, on=\"stay_id\", how='left') \n",
    "LAPS2_DF = LAPS2_DF.merge(Blood_pressure, on=\"stay_id\", how='left') \n",
    "\n",
    "# Make shock_index\n",
    "LAPS2_DF['shock_index'] = LAPS2_DF['HR'] / LAPS2_DF['blood_pressure']\n",
    "\n",
    "LAPS2_DF = LAPS2_DF.merge(Oxygen_pressure, on=\"stay_id\", how='left')\n",
    "LAPS2_DF = LAPS2_DF.merge(gcs_table, on=\"stay_id\", how='left') \n",
    "\n",
    "# Calculate missing data across each row\n",
    "LAPS2_DF['missing_counts'] = LAPS2_DF.isnull().sum(axis=1)\n",
    "LAPS2_DF['more_than_half_missing'] = (LAPS2_DF['missing_counts']>10).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Points creator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def points_assigner(df, column_name, splits, points_dict, missingness_points, high_risk_missingness_points):\n",
    "    \"\"\"\n",
    "    NOTE: The missingness and high_risk_missingness are the values to fill in\n",
    "    \"\"\"\n",
    "    column = df[column_name].copy()\n",
    "    column = pd.to_numeric(column, errors = 'coerce')\n",
    "    high_risk_col = df['high_risk']\n",
    "\n",
    "    # Assign bin labels to each\n",
    "    bin_labels = range(len(splits)-1)\n",
    "    column = pd.cut(column, bins=splits, labels=bin_labels, include_lowest=True, right=False)\n",
    "    column = column.cat.codes\n",
    "\n",
    "    # Convert this into points\n",
    "    column = column.map(points_dict)\n",
    "\n",
    "    # Assign high_risk_missingness to NA values where high_risk == 1. Fill in the points for high risk missingess\n",
    "    column.loc[(column.isnull()) & (high_risk_col == 1)] = high_risk_missingness_points\n",
    "\n",
    "    # Assign missingness to remaining NA values. Fill in the points for missingess\n",
    "    column.fillna(missingness_points, inplace=True)\n",
    "\n",
    "    return column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assign the points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Points Data Frame\n",
    "Points = LAPS2_DF[['hadm_id', 'stay_id', 'high_risk', 'death', 'missing_counts', 'more_than_half_missing', 'Arterial_pH', 'Lactate']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arterial_pH Lactate\n",
    "for index, row in LAPS2_DF.iterrows():\n",
    "    if (row['Arterial_pH'] < 7.20) and (row['Lactate'] <= 2.00):\n",
    "        Points.loc[index, 'Arterial_pH_Lactate'] = 13  # replace 'your_value' with the value you want to assign\n",
    "    \n",
    "    elif (row['Arterial_pH'] < 7.20) and (row['Lactate'] < 4.00):\n",
    "        Points.loc[index, 'Arterial_pH_Lactate'] = 19 \n",
    "\n",
    "    elif (row['Arterial_pH'] < 7.20) and (row['Lactate'] >= 4.00):\n",
    "        Points.loc[index, 'Arterial_pH_Lactate'] = 34\n",
    "\n",
    "    elif (row['Arterial_pH'] < 7.35) and (row['Lactate'] <= 2.00):\n",
    "        Points.loc[index, 'Arterial_pH_Lactate'] = 5  \n",
    "\n",
    "    elif (row['Arterial_pH'] < 7.35) and (row['Lactate'] < 4.00):\n",
    "        Points.loc[index, 'Arterial_pH_Lactate'] = 15  \n",
    "\n",
    "    elif (row['Arterial_pH'] < 7.35) and (row['Lactate'] >= 4.00):\n",
    "        Points.loc[index, 'Arterial_pH_Lactate'] = 25  \n",
    "\n",
    "    elif (row['Arterial_pH'] < 7.45) and (row['Lactate'] <= 2.00):\n",
    "        Points.loc[index, 'Arterial_pH_Lactate'] = 0  \n",
    "\n",
    "    elif (row['Arterial_pH'] < 7.45) and (row['Lactate'] < 4.00):\n",
    "        Points.loc[index, 'Arterial_pH_Lactate'] = 12  \n",
    "\n",
    "    elif (row['Arterial_pH'] < 7.45) and (row['Lactate'] >= 4.00):\n",
    "        Points.loc[index, 'Arterial_pH_Lactate'] = 26  \n",
    "\n",
    "    elif (row['Arterial_pH'] >= 7.45) and (row['Lactate'] <= 2.00):\n",
    "        Points.loc[index, 'Arterial_pH_Lactate'] = 12  \n",
    "\n",
    "    elif (row['Arterial_pH'] >= 7.45) and (row['Lactate'] < 4.00):\n",
    "        Points.loc[index, 'Arterial_pH_Lactate'] = 15 \n",
    "\n",
    "    elif (row['Arterial_pH'] >= 7.45) and (row['Lactate'] >= 4.00):\n",
    "        Points.loc[index, 'Arterial_pH_Lactate'] = 30 \n",
    "\n",
    "# Fill the rest with 0\n",
    "Points['Arterial_pH_Lactate'].fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sodium\n",
    "sodium_splits = [0, 5, 13, np.inf]\n",
    "sodium_points = {0:8, 1:0, 2:11}\n",
    "\n",
    "# Apply the points assigner\n",
    "Points['sodium'] = points_assigner(df = LAPS2_DF, column_name = 'sodium', splits = sodium_splits,\n",
    " points_dict = sodium_points, missingness_points = 0, high_risk_missingness_points = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Biblirubin\n",
    "Biblirubin_splits = [0, 2, 3, 5, 8, np.inf]\n",
    "Biblirubin_points = {0:0, 1:11, 2:18, 3:25, 4:41}\n",
    "\n",
    "# Apply the points assigner\n",
    "Points['Biblirubin'] = points_assigner(df = LAPS2_DF, column_name = 'Biblirubin', splits = Biblirubin_splits,\n",
    " points_dict = Biblirubin_points, missingness_points = 0, high_risk_missingness_points = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BUN\n",
    "BUN_splits = [0, 18, 20, 40, 80, np.inf]\n",
    "BUN_points = {0:0, 1:11, 2:12, 3:20, 4:25}\n",
    "\n",
    "# Apply the points assigner\n",
    "Points['BUN'] = points_assigner(df = LAPS2_DF, column_name = 'BUN', splits = BUN_splits,\n",
    " points_dict = BUN_points, missingness_points = 0, high_risk_missingness_points = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creatinine\n",
    "creatinine_splits = [0, 1, 2, 4, np.inf]\n",
    "creatinine_points = {0:0, 1:6, 2:11, 3:5}\n",
    "\n",
    "# Apply the points assigner\n",
    "Points['creatinine'] = points_assigner(df = LAPS2_DF, column_name = 'creatinine', splits = creatinine_splits,\n",
    " points_dict = creatinine_points, missingness_points = 0, high_risk_missingness_points = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BUN_creatinine (BUN / creatinine)\n",
    "BUN_creatinine_splits = [0, 25, np.inf]\n",
    "BUN_creatinine_points = {0:0, 1:10}\n",
    "\n",
    "# Apply the points assigner\n",
    "Points['BUN_creatinine'] = points_assigner(df = LAPS2_DF, column_name = 'BUN_creatinine', splits = BUN_creatinine_splits,\n",
    " points_dict = BUN_creatinine_points, missingness_points = 0, high_risk_missingness_points = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# albumin\n",
    "albumin_splits = [0, 2, 2.5, np.inf]\n",
    "albumin_points = {0:31, 1:15, 2:0}\n",
    "\n",
    "# Apply the points assigner\n",
    "Points['albumin'] = points_assigner(df = LAPS2_DF, column_name = 'albumin', splits = albumin_splits,\n",
    " points_dict = albumin_points, missingness_points = 0, high_risk_missingness_points = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# glucose\n",
    "glucose_splits = [0, 40, 60, 200, np.inf]\n",
    "glucose_points = {0:10, 1:10, 2:0, 3:3}\n",
    "\n",
    "# Apply the points assigner\n",
    "Points['glucose'] = points_assigner(df = LAPS2_DF, column_name = 'glucose', splits = glucose_splits,\n",
    " points_dict = glucose_points, missingness_points = 0, high_risk_missingness_points = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hematocrit\n",
    "hematocrit_splits = [0, 20, 40, 50, np.inf]\n",
    "hematocrit_points = {0:7, 1:8, 2:0, 3:3}\n",
    "\n",
    "# Apply the points assigner\n",
    "Points['hematocrit'] = points_assigner(df = LAPS2_DF, column_name = 'hematocrit', splits = hematocrit_splits,\n",
    " points_dict = hematocrit_points, missingness_points = 0, high_risk_missingness_points = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# white_blood_cells\n",
    "white_blood_cells_splits = [0, 5, 13, np.inf]\n",
    "white_blood_cells_points = {0:8, 1:0, 2:11}\n",
    "\n",
    "# Apply the points assigner\n",
    "Points['white_blood_cells'] = points_assigner(df = LAPS2_DF, column_name = 'white_blood_cells', splits = white_blood_cells_splits,\n",
    " points_dict = white_blood_cells_points, missingness_points = 0, high_risk_missingness_points = 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pCO2\n",
    "pCO2_splits = [0, 35, 45, 55, 65, np.inf]\n",
    "pCO2_points = {0:7, 1:0, 2:11, 3:13, 4:12}\n",
    "\n",
    "# Apply the points assigner\n",
    "Points['pCO2'] = points_assigner(df = LAPS2_DF, column_name = 'pCO2', splits = pCO2_splits,\n",
    " points_dict = pCO2_points, missingness_points = 0, high_risk_missingness_points = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pO2\n",
    "pO2_splits = [0, 50, 120, np.inf]\n",
    "pO2_points = {0:8, 1:0, 2:12}\n",
    "\n",
    "# Apply the points assigner\n",
    "Points['pO2'] = points_assigner(df = LAPS2_DF, column_name = 'pO2', splits = pO2_splits,\n",
    " points_dict = pO2_points, missingness_points = 0, high_risk_missingness_points = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Troponin_T\n",
    "Troponin_T_splits = [0, 0.014, 0.070, 0.3, 1, np.inf]\n",
    "Troponin_T_points = {0:0, 1:8, 2:17, 3:19, 4:25}\n",
    "\n",
    "# Apply the points assigner\n",
    "Points['Troponin_T'] = points_assigner(df = LAPS2_DF, column_name = 'Troponin_T', splits = Troponin_T_splits,\n",
    " points_dict = Troponin_T_points, missingness_points = 0, high_risk_missingness_points = 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temperature\n",
    "temperature_splits = [0, 96, 100.5, np.inf]\n",
    "temperature_points = {0:20, 1:0, 2:3}\n",
    "\n",
    "# Apply the points assigner\n",
    "Points['temperature'] = points_assigner(df = LAPS2_DF, column_name = 'temperature', splits = temperature_splits,\n",
    " points_dict = temperature_points, missingness_points = 0, high_risk_missingness_points = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HR\n",
    "HR_splits = [0, 60, 110, 140, np.inf]\n",
    "HR_points = {0:7, 1:0, 2:7, 3:10}\n",
    "\n",
    "# Apply the points assigner\n",
    "Points['HR'] = points_assigner(df = LAPS2_DF, column_name = 'HR', splits = HR_splits,\n",
    " points_dict = HR_points, missingness_points = 0, high_risk_missingness_points = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RR\n",
    "RR_splits = [0, 20, 30, np.inf]\n",
    "RR_points = {0:0, 1:11, 2:21}\n",
    "\n",
    "# Apply the points assigner\n",
    "Points['RR'] = points_assigner(df = LAPS2_DF, column_name = 'RR', splits = RR_splits,\n",
    " points_dict = RR_points, missingness_points = 0, high_risk_missingness_points = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# blood_pressure\n",
    "blood_pressure_splits = [0, 75, 90, 120, 140, 160, np.inf]\n",
    "blood_pressure_points = {0:22, 1:13, 2:5, 3:0, 4:8, 5:14}\n",
    "\n",
    "# Apply the points assigner\n",
    "Points['blood_pressure'] = points_assigner(df = LAPS2_DF, column_name = 'blood_pressure', splits = blood_pressure_splits,\n",
    " points_dict = blood_pressure_points, missingness_points = 0, high_risk_missingness_points = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shock_index\n",
    "shock_index_splits = [0, 0.65, 0.85, np.inf]\n",
    "shock_index_points = {0:0, 1:8, 2:17}\n",
    "\n",
    "# Apply the points assigner\n",
    "Points['shock_index'] = points_assigner(df = LAPS2_DF, column_name = 'shock_index', splits = shock_index_splits,\n",
    " points_dict = shock_index_points, missingness_points = 0, high_risk_missingness_points = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# oxygen\n",
    "oxygen_splits = [0, 90, 94, np.inf]\n",
    "oxygen_points = {0:22, 1:12, 2:0}\n",
    "\n",
    "# Apply the points assigner\n",
    "Points['oxygen'] = points_assigner(df = LAPS2_DF, column_name = 'oxygen', splits = oxygen_splits,\n",
    " points_dict = oxygen_points, missingness_points = 0, high_risk_missingness_points = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neurological_Score\n",
    "neuro_dict = {'Normal':0, \"Abmiguous\":16, \"Abnormal\":21, \"Very Abnormal\":36}\n",
    "neuro_df = LAPS2_DF[['high_risk', 'Neurological_Score']].copy()\n",
    "neuro_df['score'] = neuro_df['Neurological_Score'].map(neuro_dict)\n",
    "\n",
    "# Assign high_risk_missingness to NA values where high_risk == 1. Fill in the points for high risk missingess\n",
    "neuro_df.loc[(neuro_df['score'].isnull()) & (neuro_df['high_risk'] == 1), 'score'] = 21\n",
    "neuro_df.loc[(neuro_df['score'].isnull()) & (neuro_df['high_risk'] == 0), 'score'] = 16\n",
    "\n",
    "# Add back to the Points df\n",
    "Points['neuro'] = neuro_df['score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All relevant columns\n",
    "column_list = ['Arterial_pH_Lactate', 'sodium', 'Biblirubin', 'BUN', 'creatinine',\n",
    "       'BUN_creatinine', 'albumin', 'glucose', 'hematocrit',\n",
    "       'white_blood_cells', 'pCO2', 'pO2', 'Troponin_T', 'temperature', 'HR',\n",
    "       'RR', 'blood_pressure', 'shock_index', 'oxygen', 'neuro']\n",
    "\n",
    "# Add a new row with the sum of each column in the list\n",
    "Points['LAPS2'] = Points[column_list].apply(sum, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LAPS2 = Points[['hadm_id', 'LAPS2']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [7. From earlier] Predicted hospital mortality at hospital admission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assuming 'hosp' is a previously defined dictionary containing the 'admissions' dataframe\n",
    "race = hosp['admissions'][['subject_id','hadm_id', 'race']].copy()\n",
    "\n",
    "# find top 10 most popular items\n",
    "top_10 = race['race'].value_counts().nlargest(10).index\n",
    "\n",
    "# create dummies for top 10 most popular items\n",
    "dummies = pd.get_dummies(race['race']).loc[:, top_10].astype(int)\n",
    "\n",
    "# concatenate the original dataframe with the dummy dataframe\n",
    "race = pd.concat([race, dummies], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a neat and minimmal dataframe\n",
    "Mortality_Predictor = LAPS2_DF[['subject_id', 'hadm_id', 'stay_id', 'deathin30']].copy()\n",
    "Mortality_Predictor = Mortality_Predictor.merge(Points[['stay_id', 'LAPS2']], on='stay_id', how = 'left')\n",
    "\n",
    "# Merge in the demographics\n",
    "Mortality_Predictor = Mortality_Predictor.merge(derived_age, on='hadm_id', how = 'left')\n",
    "Mortality_Predictor = Mortality_Predictor.merge(gender, on='subject_id', how = 'left')\n",
    "Mortality_Predictor = Mortality_Predictor.merge(CCI, on='hadm_id', how = 'left')\n",
    "Mortality_Predictor = Mortality_Predictor.merge(race[['hadm_id', 'WHITE', 'BLACK/AFRICAN AMERICAN',\n",
    "       'OTHER', 'UNKNOWN', 'HISPANIC/LATINO - PUERTO RICAN',\n",
    "       'WHITE - OTHER EUROPEAN', 'HISPANIC OR LATINO', 'ASIAN',\n",
    "       'ASIAN - CHINESE', 'WHITE - RUSSIAN']], on='hadm_id', how = 'left')\n",
    "\n",
    "# Rename the race columns\n",
    "Mortality_Predictor.rename(columns = {'WHITE':'Race_1', 'BLACK/AFRICAN AMERICAN': 'Race_2', 'OTHER':'Race_3',\n",
    "'UNKNOWN':'Race_4', 'HISPANIC/LATINO - PUERTO RICAN':'Race_5', 'WHITE - OTHER EUROPEAN':'Race_6', \n",
    "'HISPANIC OR LATINO':'Race_7', 'ASIAN':'Race_8', 'ASIAN - CHINESE':'Race_9', 'WHITE - RUSSIAN':'Race_10'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Age, sex, race, comorbidities, comorbidities*LAPS2, LAPS2, LAPS2^2\n",
    "# Concious decision to remove [charlson_comorbidity_index + charlson_comorbidity_index*LAPS2] terms \n",
    "# These contain data based on billed diagnoses so result could be a little missleading\n",
    "\n",
    "formula_1 = \"\"\"deathin30 ~ age + gender + Race_1 + Race_2 + Race_3 + Race_4 + Race_5 + Race_6 + Race_7 + \n",
    "Race_8 + Race_9 + Race_10 + LAPS2 + LAPS2*LAPS2\"\"\"\n",
    "\n",
    "fit = smf.logit(formula = formula_1, data = Mortality_Predictor).fit()\n",
    "\n",
    "predictions = fit.predict()\n",
    "\n",
    "# Binarizing the predictions\n",
    "predictions_binary = (predictions > 0.5).astype(int)\n",
    "\n",
    "# Getting actual labels\n",
    "actual_labels = Mortality_Predictor['deathin30']\n",
    "\n",
    "# calculating F1 score\n",
    "f1 = f1_score(actual_labels, predictions_binary)\n",
    "\n",
    "# Make a trim dataset\n",
    "LAPS2_complete = Mortality_Predictor[['hadm_id', 'LAPS2']].copy()\n",
    "LAPS2_complete['Mortality_prob'] = predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce to smaller dataset\n",
    "mortality_prob = LAPS2_complete[['hadm_id', 'Mortality_prob']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Putting it all together "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base data from the icustays table\n",
    "input_data = icu['icustays'][['stay_id', 'hadm_id', 'subject_id']]\n",
    "\n",
    "# DERIVED_AGE + column for over 89\n",
    "input_data = input_data.merge(derived_age, on=\"hadm_id\", how='left')\n",
    "\n",
    "# Merge in comorbidiy disease - CCI_export\n",
    "input_data = input_data.merge(CCI, on=\"hadm_id\", how='left')\n",
    "\n",
    "# Merge in admission source \n",
    "input_data = input_data.merge(admission_measures, on=\"hadm_id\", how='left')\n",
    "\n",
    "# Merge in need for surgical or procedural intervention \n",
    "input_data = input_data.merge(surgery_out, on=\"hadm_id\", how='left')\n",
    "\n",
    "# Merge in severity of illness at hospital admission\n",
    "input_data = input_data.merge(LAPS2, on=\"hadm_id\", how='left')\n",
    "\n",
    "# Merge in code status code_details.\n",
    "input_data = input_data.merge(code_details, on=\"stay_id\", how='left')\n",
    "input_data['Full_code'] = input_data['Full_code'].fillna(1.0)\n",
    "input_data['DNR'] = input_data['DNR'].fillna(0.0)\n",
    "\n",
    "# Merge in predicted hospital mortality at hospital admission\n",
    "input_data = input_data.merge(mortality_prob, on=\"hadm_id\", how='left')\n",
    "\n",
    "# Merge in severity of illness of ICU admission - sapsii\n",
    "input_data = input_data.merge(sapsii, on=\"stay_id\", how='left')\n",
    "\n",
    "# Merge in days receiving benzodiazepines. Fill NAs with \n",
    "input_data = input_data.merge(benzos_info, on=\"stay_id\", how='left')\n",
    "input_data.benzos_days = input_data.benzos_days.fillna(0.0)\n",
    "\n",
    "# Merge in days receiving other sedatives / non-benzodiazepines\n",
    "input_data = input_data.merge(seds_info, on=\"stay_id\", how='left')\n",
    "input_data.seds_days = input_data.seds_days.fillna(0.0)\n",
    "\n",
    "# Merge in days receiving opiates \n",
    "input_data = input_data.merge(opiates_info, on=\"stay_id\", how='left')\n",
    "input_data.opiate_days = input_data.opiate_days.fillna(0.0)\n",
    "\n",
    "# Merge in hospital LOS. Note that I am merging on hadm_id not stay_id as hospitalisation statistic\n",
    "input_data = input_data.merge(hosp_los[['hadm_id', 'hosp_LOS']], on=\"hadm_id\", how='left')\n",
    "\n",
    "# Merge in vital status at discharge. Note that I am merging on hadm_id not stay_id as hospitalisation statistic\n",
    "input_data = input_data.merge(vital_status_discharge[['hadm_id', 'vital_status_discharge']], on=\"hadm_id\", how='left')\n",
    "\n",
    "# Merge in vital status 30 days after admission. Note that I am merging on hadm_id not stay_id as hospitalisation statistic\n",
    "input_data = input_data.merge(deathin30_df[['hadm_id', 'deathin30']], on=\"hadm_id\", how='left')\n",
    "\n",
    "# Merge in discharge location. Note that I am merging on hadm_id not stay_id as hospitalisation statistic.\n",
    "input_data = input_data.merge(discharge_locations[['hadm_id', 'discharged_home', 'discharged_hospice', 'discharged_skilled_facility']], on=\"hadm_id\", how='left')\n",
    "\n",
    "# Merge in hospital readmission within 30 days of discharge from hospital. Note that I am merging on hadm_id not stay_id as hospitalisation statistic\n",
    "input_data = input_data.merge(readmission_df[['hadm_id', 'readmissionin30']], on=\"hadm_id\", how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check and filter out patients we don't want to include in the dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data for filtering\n",
    "data_for_filtering = input_data.copy()\n",
    "\n",
    "# Check ages\n",
    "print(f\"There are {len(data_for_filtering[data_for_filtering.age < 18])} patients under 18.\")\n",
    "\n",
    "# Filter out OBS \n",
    "filtered_data = data_for_filtering[~data_for_filtering.hadm_id.isin(Obstetrical_hadm_ids)].reset_index(drop = True)\n",
    "print(f\"There are {len(data_for_filtering) - len(filtered_data)} obstetrical patients in the dataset. Now removed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export the final dataset to your data directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter and rearrange columns\n",
    "Clustering_Data = filtered_data[['stay_id', 'age', 'charlson_comorbidity_index',\n",
    "'ED_Admission', 'Surgery_Admission', 'LAPS2', 'Full_code', 'DNR', 'Mortality_prob', 'sapsii', 'benzos_days', 'seds_days',\n",
    "'opiate_days', 'hosp_LOS', 'vital_status_discharge', 'deathin30', 'discharged_home', 'discharged_hospice',\n",
    "'discharged_skilled_facility', 'readmissionin30']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to csv\n",
    "Clustering_Data.to_csv(\"data/cleaned_data.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12.0 ('main_venv_312')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cdd410d891f8f32b9515ba93828e5c127a6a5caaad336530b204d0f41e436244"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
